{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVIaqrkAa9qtnXYGnqmCkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TKing151/TextClassification_Survey/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text classification on scikit's newsgroup text dataset. Multinomial Naive Bayes model."
      ],
      "metadata": {
        "id": "Ix7Dh5Ract-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.misc', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train a Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict categories for the test data\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "f1 = f1_score(y_test, predictions, average='weighted')  # 'weighted' takes class imbalance into account\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "macro_f1 = f1_score(y_test, predictions, average='macro')\n",
        "print(\"Macro F1-score:\", macro_f1)\n",
        "\n",
        "# Calculate micro F1-score\n",
        "micro_f1 = f1_score(y_test, predictions, average='micro')\n",
        "print(\"Micro F1-score:\", micro_f1)\n",
        "\n",
        "# Calculate weighted F1-score\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Weighted F1-score:\", weighted_f1)\n",
        "\n",
        "# Calculate Average Precision Score\n",
        "#avg_precision = average_precision_score(y_test, model.predict_proba(X_test_tfidf), average='macro')\n",
        "#print(\"Average Precision Score:\", avg_precision)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NmuyuwwHPvS",
        "outputId": "16ee3e2d-d052-4f22-db80-31fcee012187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7805953693495039\n",
            "F1-score: 0.7787313881379052\n",
            "Macro F1-score: 0.7699958618698883\n",
            "Micro F1-score: 0.780595369349504\n",
            "Weighted F1-score: 0.7787313881379052\n",
            "Confusion Matrix:\n",
            " [[109   5  13  10   9]\n",
            " [  5 180   5   7   1]\n",
            " [ 12  10 174  16   4]\n",
            " [  8  16  10 153   5]\n",
            " [ 32   3   8  20  92]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuned Hyper-parameters"
      ],
      "metadata": {
        "id": "eC2sy1zRfk8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Define hyperparameter grid for grid search\n",
        "param_grid = {'alpha': [0.1, 0.4, .9, 2.0]}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted') #'f1_macro')\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "predictions = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate macro F1-score of the best model\n",
        "macro_f1 = f1_score(y_test, predictions, average='macro')\n",
        "print(\"Best Model Macro F1-score:\", macro_f1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "f1 = f1_score(y_test, predictions, average='weighted')  # 'weighted' takes class imbalance into account\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "macro_f1 = f1_score(y_test, predictions, average='macro')\n",
        "print(\"Macro F1-score:\", macro_f1)\n",
        "\n",
        "# Calculate micro F1-score\n",
        "micro_f1 = f1_score(y_test, predictions, average='micro')\n",
        "print(\"Micro F1-score:\", micro_f1)\n",
        "\n",
        "# Calculate weighted F1-score\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Weighted F1-score:\", weighted_f1)\n",
        "\n",
        "# Calculate Average Precision Score\n",
        "#avg_precision = average_precision_score(y_test, model.predict_proba(X_test_tfidf), average='macro')\n",
        "#print(\"Average Precision Score:\", avg_precision)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3VwyfrZXy1V",
        "outputId": "24706709-7949-4f32-9058-462661b3a466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'alpha': 0.1}\n",
            "Best Model Macro F1-score: 0.8444349931608249\n",
            "Accuracy: 0.8457446808510638\n",
            "F1-score: 0.8453748141646517\n",
            "Macro F1-score: 0.8444349931608249\n",
            "Micro F1-score: 0.8457446808510638\n",
            "Weighted F1-score: 0.8453748141646517\n",
            "Confusion Matrix:\n",
            " [[129   5  14  14]\n",
            " [  6 165  12   9]\n",
            " [  6   7 179   7]\n",
            " [ 11  13  12 163]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnb with lemmatization"
      ],
      "metadata": {
        "id": "Q8WEi6eCBRQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU1fGK3_Be7u",
        "outputId": "d2b6778c-e0ca-4d37-9518-9ecb36d40640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lemmatization function using NLTK\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the text data\n",
        "X_train_lemmatized = [lemmatize_text(text) for text in X_train]\n",
        "X_test_lemmatized = [lemmatize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lemmatized data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
        "\n",
        "# Define hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "# Create MultinomialNB model\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted')\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"Test Weighted F1-score:\", test_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ5D1BMQBUMF",
        "outputId": "45d8b634-df02-4d67-c936-ed4fec77fc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'alpha': 0.1, 'fit_prior': True}\n",
            "Test Weighted F1-score: 0.8547694327523531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "switch models to logistic regression"
      ],
      "metadata": {
        "id": "r1BalCIKftcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a Logistic Regression classifier for multiclass classification\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OlgivRbfs_e",
        "outputId": "e20231b1-995f-431a-fc9a-3eb490b1c070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weighted F1-score: 0.8166253018639785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tune hyper-parameters for logistic regression"
      ],
      "metadata": {
        "id": "InQeCHbZhSAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a Logistic Regression classifier for multiclass classification\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_weighted')\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "predictions = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the best model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Best Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKSciBeEhNgU",
        "outputId": "c866fd2a-9488-4f41-cc52-ab2752fbc6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 1}\n",
            "Best Model Weighted F1-score: 0.8166253018639785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "switch to KNN"
      ],
      "metadata": {
        "id": "PrjEadJyiKjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a k-Nearest Neighbors classifier\n",
        "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltT-Kd6QiMeD",
        "outputId": "818250c0-c0fb-43de-82b4-bb663b287113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weighted F1-score: 0.4197603328155643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN with grid search"
      ],
      "metadata": {
        "id": "ctvtDXgVAOio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a k-Nearest Neighbors classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9]  # Test different numbers of neighbors\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted')\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "predictions = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the best model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Best Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGSALGy2i4fe",
        "outputId": "0d9fd89e-f0a0-439f-9964-e99f73e2b4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_neighbors': 1}\n",
            "Best Model Weighted F1-score: 0.44363394816344753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost without tuning"
      ],
      "metadata": {
        "id": "8acouolBkqXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize an XGBoost classifier with default hyperparameters\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "xgb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = xgb.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5lm8g-0kXXW",
        "outputId": "a396aec5-e021-4ddf-c77a-ccf8e4a55cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weighted F1-score: 0.7929220659716383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "xg boost with some tuning and lemmatization"
      ],
      "metadata": {
        "id": "zWJs5JLm94_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lemmatization function using NLTK\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the text data\n",
        "X_train_lemmatized = [lemmatize_text(text) for text in X_train]\n",
        "X_test_lemmatized = [lemmatize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lemmatized data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
        "\n",
        "# Define hyperparameter ranges for random search\n",
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_child_weight': [1, 2, 3, 4],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Perform random search\n",
        "n_iter_search = 10 # 50\n",
        "best_f1 = 0\n",
        "best_params = None\n",
        "\n",
        "for _ in range(n_iter_search):\n",
        "    params = {key: np.random.choice(values) for key, values in param_dist.items()}\n",
        "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)), **params)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best F1-score:\", best_f1)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VSOJH7s965u",
        "outputId": "b63d1332-34b3-42a9-f7c5-736c9e7d05ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1-score: 0.8132935151768679\n",
            "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 4, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "GJjnSEk4qfrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate weighted F1-score of the model\n",
        "weighted_f1 = f1_score(y_test, predictions, average='weighted')\n",
        "print(\"Model Weighted F1-score:\", weighted_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zmQHBD65VMH",
        "outputId": "99a00b41-498a-477c-c2d6-3338a8e205b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weighted F1-score: 0.7582243111520194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest with some tuning and lemmatization"
      ],
      "metadata": {
        "id": "h0oPdEcQA_kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lemmatization function using NLTK\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the text data\n",
        "X_train_lemmatized = [lemmatize_text(text) for text in X_train]\n",
        "X_test_lemmatized = [lemmatize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lemmatized data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
        "\n",
        "# Define hyperparameter ranges for random search\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform random search\n",
        "n_iter_search = 50\n",
        "best_f1 = 0\n",
        "best_params = None\n",
        "\n",
        "for _ in range(n_iter_search):\n",
        "    params = {key: np.random.choice(values) for key, values in param_dist.items()}\n",
        "    model = RandomForestClassifier(**params)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best F1-score:\", best_f1)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_NgrzY_BC-j",
        "outputId": "80d0914e-2738-4469-b2e1-55373df84a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1-score: 0.8090265170144113\n",
            "Best Hyperparameters: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch NN without optimization"
      ],
      "metadata": {
        "id": "zJAfgvrF8FUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tfidf = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "X_test_tfidf = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Define a simple neural network model\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train_tfidf.shape[1]\n",
        "output_dim = len(categories)\n",
        "model = TextClassifier(input_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tfidf)\n",
        "    loss = criterion(outputs, y_train_encoded)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test_tfidf)\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "    test_f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')\n",
        "    print(\"Test Weighted F1-score:\", test_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9a9Wr298CWF",
        "outputId": "9f20ef6a-7317-4bad-840e-b4701c8c4441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3873318433761597\n",
            "Epoch [2/10], Loss: 1.3848925828933716\n",
            "Epoch [3/10], Loss: 1.3824602365493774\n",
            "Epoch [4/10], Loss: 1.380035161972046\n",
            "Epoch [5/10], Loss: 1.3776167631149292\n",
            "Epoch [6/10], Loss: 1.375205636024475\n",
            "Epoch [7/10], Loss: 1.372801423072815\n",
            "Epoch [8/10], Loss: 1.3704041242599487\n",
            "Epoch [9/10], Loss: 1.3680142164230347\n",
            "Epoch [10/10], Loss: 1.3656309843063354\n",
            "Test Weighted F1-score: 0.3906516277983329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimized NN"
      ],
      "metadata": {
        "id": "AMYoEtin8rrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGbsvYTm8uLp",
        "outputId": "1725b4b4-062a-4e12-f479-14e4e5dc7411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.3-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.10.0 (from optuna)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.3 cmaes-0.10.0 colorlog-6.7.0 optuna-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import optuna\n",
        "\n",
        "# Load the 20 Newsgroups dataset (news articles classified into categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tfidf = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.long)\n",
        "X_test_tfidf = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Define a simple neural network model\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_hidden, dropout_rate):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, n_hidden)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(n_hidden, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define objective function for Optuna\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "    n_hidden = trial.suggest_int('n_hidden', 16, 256, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
        "\n",
        "    model = TextClassifier(input_dim, output_dim, n_hidden, dropout_rate)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_tfidf)\n",
        "        loss = criterion(outputs, y_train_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_outputs = model(X_test_tfidf)\n",
        "        predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "        test_f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')\n",
        "\n",
        "    return 1.0 - test_f1  # Optuna minimizes the objective function\n",
        "\n",
        "# Perform hyperparameter tuning using Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x-SxNdp807J",
        "outputId": "3cef5afd-034f-4555-9b61-63e693d77cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-28 02:23:34,581] A new study created in memory with name: no-name-dd349e58-69d2-45cf-b6b6-b2ee8f4d2677\n",
            "[I 2023-08-28 02:23:35,704] Trial 0 finished with value: 0.9183819722979046 and parameters: {'learning_rate': 7.278203006709827e-05, 'n_hidden': 163, 'dropout_rate': 0.43901518036512166}. Best is trial 0 with value: 0.9183819722979046.\n",
            "[I 2023-08-28 02:23:36,914] Trial 1 finished with value: 0.5769786292465385 and parameters: {'learning_rate': 0.00014181360841297754, 'n_hidden': 208, 'dropout_rate': 0.4126617806087691}. Best is trial 1 with value: 0.5769786292465385.\n",
            "[I 2023-08-28 02:23:38,150] Trial 2 finished with value: 0.2284185182780153 and parameters: {'learning_rate': 0.004820909694630673, 'n_hidden': 118, 'dropout_rate': 0.4885984977960584}. Best is trial 2 with value: 0.2284185182780153.\n",
            "[I 2023-08-28 02:23:38,957] Trial 3 finished with value: 0.8467111667361981 and parameters: {'learning_rate': 0.00020049616954438115, 'n_hidden': 113, 'dropout_rate': 0.40747879985962443}. Best is trial 2 with value: 0.2284185182780153.\n",
            "[I 2023-08-28 02:23:39,732] Trial 4 finished with value: 0.8892515157616843 and parameters: {'learning_rate': 2.6219016690276386e-05, 'n_hidden': 99, 'dropout_rate': 0.48606106779092034}. Best is trial 2 with value: 0.2284185182780153.\n",
            "[I 2023-08-28 02:23:40,239] Trial 5 finished with value: 0.1673100207019491 and parameters: {'learning_rate': 0.03445755754688564, 'n_hidden': 63, 'dropout_rate': 0.3278486441853503}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:40,698] Trial 6 finished with value: 0.8961413631446087 and parameters: {'learning_rate': 4.743182293266307e-05, 'n_hidden': 24, 'dropout_rate': 0.30719454067689594}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:41,567] Trial 7 finished with value: 0.9238561786206413 and parameters: {'learning_rate': 5.204602690738653e-05, 'n_hidden': 52, 'dropout_rate': 0.2592539545943746}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:42,582] Trial 8 finished with value: 0.8892515157616843 and parameters: {'learning_rate': 4.8966402733962754e-05, 'n_hidden': 64, 'dropout_rate': 0.494112986545183}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:43,342] Trial 9 finished with value: 0.8892515157616843 and parameters: {'learning_rate': 0.000360061229877541, 'n_hidden': 39, 'dropout_rate': 0.39894746933382985}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:43,782] Trial 10 finished with value: 0.178448735341578 and parameters: {'learning_rate': 0.04704605868913623, 'n_hidden': 16, 'dropout_rate': 0.20110768126571066}. Best is trial 5 with value: 0.1673100207019491.\n",
            "[I 2023-08-28 02:23:44,110] Trial 11 finished with value: 0.1621777994950846 and parameters: {'learning_rate': 0.07926441605685919, 'n_hidden': 16, 'dropout_rate': 0.2043830562057708}. Best is trial 11 with value: 0.1621777994950846.\n",
            "[I 2023-08-28 02:23:44,657] Trial 12 finished with value: 0.15816804607464485 and parameters: {'learning_rate': 0.08382372228112941, 'n_hidden': 31, 'dropout_rate': 0.32907381707344047}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:44,849] Trial 13 finished with value: 0.17178734510659466 and parameters: {'learning_rate': 0.0670667849401356, 'n_hidden': 16, 'dropout_rate': 0.20390128965980267}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:45,209] Trial 14 finished with value: 0.2405825875381984 and parameters: {'learning_rate': 0.010560334936412188, 'n_hidden': 25, 'dropout_rate': 0.24786273939525671}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:45,492] Trial 15 finished with value: 0.1624015246697994 and parameters: {'learning_rate': 0.08285910892839875, 'n_hidden': 27, 'dropout_rate': 0.3469276622862321}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:45,767] Trial 16 finished with value: 0.24850515967310982 and parameters: {'learning_rate': 0.0022618230830602835, 'n_hidden': 31, 'dropout_rate': 0.29308108640660757}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:45,988] Trial 17 finished with value: 0.18959467571434008 and parameters: {'learning_rate': 0.016849211238127406, 'n_hidden': 21, 'dropout_rate': 0.3679754246496389}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:46,299] Trial 18 finished with value: 0.18986098375386218 and parameters: {'learning_rate': 0.017297388487285372, 'n_hidden': 37, 'dropout_rate': 0.2782041533353151}. Best is trial 12 with value: 0.15816804607464485.\n",
            "[I 2023-08-28 02:23:46,506] Trial 19 finished with value: 0.1554582706725821 and parameters: {'learning_rate': 0.08577223481795226, 'n_hidden': 18, 'dropout_rate': 0.23717436776741105}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:46,734] Trial 20 finished with value: 0.8264760492378371 and parameters: {'learning_rate': 0.0009791851167533041, 'n_hidden': 21, 'dropout_rate': 0.31744146481542285}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:46,978] Trial 21 finished with value: 0.16383509940590846 and parameters: {'learning_rate': 0.0925690662365369, 'n_hidden': 16, 'dropout_rate': 0.2424021859062864}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:47,604] Trial 22 finished with value: 0.1809839465839721 and parameters: {'learning_rate': 0.02993685251084357, 'n_hidden': 20, 'dropout_rate': 0.22602427999003843}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:48,026] Trial 23 finished with value: 0.1580533030997796 and parameters: {'learning_rate': 0.09635230703171083, 'n_hidden': 33, 'dropout_rate': 0.27343894231955895}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:48,292] Trial 24 finished with value: 0.18600069939335917 and parameters: {'learning_rate': 0.030017737267059016, 'n_hidden': 32, 'dropout_rate': 0.27836244937072396}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:48,620] Trial 25 finished with value: 0.15678459893619523 and parameters: {'learning_rate': 0.09890392852298364, 'n_hidden': 44, 'dropout_rate': 0.2723859106950704}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:49,166] Trial 26 finished with value: 0.23922327260965226 and parameters: {'learning_rate': 0.007361529608558047, 'n_hidden': 43, 'dropout_rate': 0.2640292218915139}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:49,733] Trial 27 finished with value: 0.164659117395498 and parameters: {'learning_rate': 0.038162915105870256, 'n_hidden': 46, 'dropout_rate': 0.2884007007061493}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:50,078] Trial 28 finished with value: 0.18177618737623302 and parameters: {'learning_rate': 0.016872686239038554, 'n_hidden': 36, 'dropout_rate': 0.23451780861167193}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:50,454] Trial 29 finished with value: 0.1699063908765326 and parameters: {'learning_rate': 0.0434718191402144, 'n_hidden': 51, 'dropout_rate': 0.26335818811145484}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:50,843] Trial 30 finished with value: 0.15807941027031758 and parameters: {'learning_rate': 0.09747263777133838, 'n_hidden': 26, 'dropout_rate': 0.2254629490809616}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:51,135] Trial 31 finished with value: 0.16095248916026306 and parameters: {'learning_rate': 0.08701835847684536, 'n_hidden': 27, 'dropout_rate': 0.2303418668030527}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:51,436] Trial 32 finished with value: 0.16212067956009613 and parameters: {'learning_rate': 0.0526372499171827, 'n_hidden': 34, 'dropout_rate': 0.22071859213642242}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:51,771] Trial 33 finished with value: 0.18107557534724517 and parameters: {'learning_rate': 0.0230407800434617, 'n_hidden': 28, 'dropout_rate': 0.25311950274063705}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:52,017] Trial 34 finished with value: 0.16900628001068685 and parameters: {'learning_rate': 0.04777683924375479, 'n_hidden': 22, 'dropout_rate': 0.27476195190220987}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:52,875] Trial 35 finished with value: 0.18475213118021916 and parameters: {'learning_rate': 0.009694688238094194, 'n_hidden': 255, 'dropout_rate': 0.24420521307281537}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:53,011] Trial 36 finished with value: 0.1893081278187927 and parameters: {'learning_rate': 0.024500921666284484, 'n_hidden': 24, 'dropout_rate': 0.2989220262246692}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:53,146] Trial 37 finished with value: 0.17544414008128195 and parameters: {'learning_rate': 0.05178879847495768, 'n_hidden': 20, 'dropout_rate': 0.22104229641088166}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:53,308] Trial 38 finished with value: 0.35575914235079875 and parameters: {'learning_rate': 0.005142044077092284, 'n_hidden': 30, 'dropout_rate': 0.26927348728126954}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:53,478] Trial 39 finished with value: 0.16599511592736593 and parameters: {'learning_rate': 0.05249625542925306, 'n_hidden': 37, 'dropout_rate': 0.25322329759716594}. Best is trial 19 with value: 0.1554582706725821.\n",
            "[I 2023-08-28 02:23:53,727] Trial 40 finished with value: 0.15196591925811276 and parameters: {'learning_rate': 0.0907641447977864, 'n_hidden': 62, 'dropout_rate': 0.2902043804815964}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:53,992] Trial 41 finished with value: 0.1633030185837343 and parameters: {'learning_rate': 0.09206045016747295, 'n_hidden': 73, 'dropout_rate': 0.2893256538558968}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:54,171] Trial 42 finished with value: 0.1570474254447135 and parameters: {'learning_rate': 0.09522551827379663, 'n_hidden': 42, 'dropout_rate': 0.30704384844198107}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:54,348] Trial 43 finished with value: 0.17154329498943488 and parameters: {'learning_rate': 0.032841924355393175, 'n_hidden': 42, 'dropout_rate': 0.30708607795986115}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:54,549] Trial 44 finished with value: 0.16314089833947232 and parameters: {'learning_rate': 0.05381430622392261, 'n_hidden': 52, 'dropout_rate': 0.3053457428994157}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:54,865] Trial 45 finished with value: 0.15415331858630088 and parameters: {'learning_rate': 0.06469664142896389, 'n_hidden': 62, 'dropout_rate': 0.2825728896672646}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:55,309] Trial 46 finished with value: 0.17025291343086735 and parameters: {'learning_rate': 0.061347679574306625, 'n_hidden': 84, 'dropout_rate': 0.31519401069377345}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:55,680] Trial 47 finished with value: 0.16206562446638995 and parameters: {'learning_rate': 0.03344389402657223, 'n_hidden': 59, 'dropout_rate': 0.2832235682776872}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:56,111] Trial 48 finished with value: 0.16297794664327592 and parameters: {'learning_rate': 0.06286042024107205, 'n_hidden': 69, 'dropout_rate': 0.25980133058112764}. Best is trial 40 with value: 0.15196591925811276.\n",
            "[I 2023-08-28 02:23:56,450] Trial 49 finished with value: 0.18257944385632097 and parameters: {'learning_rate': 0.021828829897039985, 'n_hidden': 57, 'dropout_rate': 0.2965482757101786}. Best is trial 40 with value: 0.15196591925811276.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.0907641447977864, 'n_hidden': 62, 'dropout_rate': 0.2902043804815964}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best hyperparameters to train and evaluate the model\n",
        "best_learning_rate = study.best_params['learning_rate']\n",
        "best_n_hidden = study.best_params['n_hidden']\n",
        "best_dropout_rate = study.best_params['dropout_rate']\n",
        "\n",
        "# Initialize the model with the best hyperparameters\n",
        "best_model = TextClassifier(input_dim, output_dim, best_n_hidden, best_dropout_rate)\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = best_model(X_train_tfidf)\n",
        "    loss = criterion(outputs, y_train_encoded)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    best_model.eval()\n",
        "    test_outputs = best_model(X_test_tfidf)\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "    test_f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')\n",
        "    print(\"Test Weighted F1-score:\", test_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvIaXJut9vZq",
        "outputId": "6d07b247-4bb4-483f-e0f2-afb1c437af0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.385244607925415\n",
            "Epoch [2/10], Loss: 1.3104469776153564\n",
            "Epoch [3/10], Loss: 1.0459202527999878\n",
            "Epoch [4/10], Loss: 0.7829515933990479\n",
            "Epoch [5/10], Loss: 0.5490607023239136\n",
            "Epoch [6/10], Loss: 0.4481745660305023\n",
            "Epoch [7/10], Loss: 0.37011855840682983\n",
            "Epoch [8/10], Loss: 0.31552624702453613\n",
            "Epoch [9/10], Loss: 0.27936795353889465\n",
            "Epoch [10/10], Loss: 0.23024460673332214\n",
            "Test Weighted F1-score: 0.8410137546827419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatized NN"
      ],
      "metadata": {
        "id": "9_3xMcWADNrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lemmatization function using NLTK\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the text data\n",
        "X_train_lemmatized = [lemmatize_text(text) for text in X_train]\n",
        "X_test_lemmatized = [lemmatize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lemmatized data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
        "\n",
        "# Define the PyTorch neural network architecture\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, dropout_rate, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(input_size, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    input_size = X_train_tfidf.shape[1]\n",
        "    output_size = len(np.unique(y_train))\n",
        "    n_hidden = trial.suggest_int(\"n_hidden\", 32, 128)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
        "\n",
        "    model = NeuralNetwork(input_size, n_hidden, dropout_rate, output_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    epochs = 10\n",
        "    batch_size = 64\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i in range(0, X_train_tfidf.shape[0], batch_size):\n",
        "            inputs = torch.FloatTensor(X_train_tfidf[i:i+batch_size].toarray())\n",
        "            labels = torch.LongTensor(y_train[i:i+batch_size])\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = []\n",
        "        for i in range(0, X_train_tfidf.shape[0], batch_size):\n",
        "#for i in range(0, len(X_test_tfidf), batch_size):\n",
        "            inputs = torch.FloatTensor(X_test_tfidf[i:i+batch_size].toarray())\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "    return -f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Perform hyperparameter tuning with Optuna\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the optimization results\n",
        "print(\"Best Trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"Value: \", trial.value)\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG9CMsotDQJO",
        "outputId": "0ab7f095-001f-44eb-82e4-2bb608071125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-28 02:54:42,981] A new study created in memory with name: no-name-260e2146-49bb-40dc-9aff-83107e803283\n",
            "[I 2023-08-28 02:54:44,245] Trial 0 finished with value: -0.85511950986777 and parameters: {'n_hidden': 93, 'dropout_rate': 0.4381659551759679}. Best is trial 0 with value: -0.85511950986777.\n",
            "[I 2023-08-28 02:54:45,438] Trial 1 finished with value: -0.8538026605869927 and parameters: {'n_hidden': 73, 'dropout_rate': 0.3221388486931024}. Best is trial 0 with value: -0.85511950986777.\n",
            "[I 2023-08-28 02:54:46,681] Trial 2 finished with value: -0.857813334789979 and parameters: {'n_hidden': 93, 'dropout_rate': 0.23103936619901347}. Best is trial 2 with value: -0.857813334789979.\n",
            "[I 2023-08-28 02:54:47,872] Trial 3 finished with value: -0.8565371801297004 and parameters: {'n_hidden': 73, 'dropout_rate': 0.4192982865046845}. Best is trial 2 with value: -0.857813334789979.\n",
            "[I 2023-08-28 02:54:49,683] Trial 4 finished with value: -0.8550947966629746 and parameters: {'n_hidden': 127, 'dropout_rate': 0.48654415781828003}. Best is trial 2 with value: -0.857813334789979.\n",
            "[I 2023-08-28 02:54:51,680] Trial 5 finished with value: -0.8564317906310722 and parameters: {'n_hidden': 116, 'dropout_rate': 0.49837301739089657}. Best is trial 2 with value: -0.857813334789979.\n",
            "[I 2023-08-28 02:54:53,024] Trial 6 finished with value: -0.8590518068830231 and parameters: {'n_hidden': 108, 'dropout_rate': 0.34834603616050186}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:54:54,283] Trial 7 finished with value: -0.8563559821274004 and parameters: {'n_hidden': 94, 'dropout_rate': 0.20549007034829603}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:54:55,190] Trial 8 finished with value: -0.8551984324258355 and parameters: {'n_hidden': 33, 'dropout_rate': 0.44496880340092476}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:54:56,566] Trial 9 finished with value: -0.8552066810668981 and parameters: {'n_hidden': 110, 'dropout_rate': 0.3695328224396992}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:54:58,547] Trial 10 finished with value: -0.8577940840836072 and parameters: {'n_hidden': 55, 'dropout_rate': 0.32397633694733174}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:55:00,778] Trial 11 finished with value: -0.8564009997446438 and parameters: {'n_hidden': 97, 'dropout_rate': 0.24770301954950252}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:55:03,276] Trial 12 finished with value: -0.8565258750875147 and parameters: {'n_hidden': 108, 'dropout_rate': 0.28580535109609106}. Best is trial 6 with value: -0.8590518068830231.\n",
            "[I 2023-08-28 02:55:05,011] Trial 13 finished with value: -0.8591808107394762 and parameters: {'n_hidden': 84, 'dropout_rate': 0.20279539641188052}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:06,118] Trial 14 finished with value: -0.8578161146418295 and parameters: {'n_hidden': 64, 'dropout_rate': 0.26682816744508864}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:07,306] Trial 15 finished with value: -0.8590640620466394 and parameters: {'n_hidden': 80, 'dropout_rate': 0.20076748324171392}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:08,347] Trial 16 finished with value: -0.8578614579623409 and parameters: {'n_hidden': 52, 'dropout_rate': 0.20208878397967886}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:09,524] Trial 17 finished with value: -0.8551402687655969 and parameters: {'n_hidden': 82, 'dropout_rate': 0.2321424508821649}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:10,737] Trial 18 finished with value: -0.8591414117564603 and parameters: {'n_hidden': 81, 'dropout_rate': 0.27564201084518164}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:11,763] Trial 19 finished with value: -0.8538248017388668 and parameters: {'n_hidden': 44, 'dropout_rate': 0.2784847105859397}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:12,955] Trial 20 finished with value: -0.8550930093962129 and parameters: {'n_hidden': 67, 'dropout_rate': 0.24897286505304098}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:14,149] Trial 21 finished with value: -0.857872076860404 and parameters: {'n_hidden': 80, 'dropout_rate': 0.20466002664904495}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:15,665] Trial 22 finished with value: -0.8591216597477115 and parameters: {'n_hidden': 85, 'dropout_rate': 0.23109667479347526}. Best is trial 13 with value: -0.8591808107394762.\n",
            "[I 2023-08-28 02:55:17,458] Trial 23 finished with value: -0.8604637125671057 and parameters: {'n_hidden': 84, 'dropout_rate': 0.2892207850595626}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:18,858] Trial 24 finished with value: -0.8578721132259522 and parameters: {'n_hidden': 87, 'dropout_rate': 0.2974509794362772}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:20,070] Trial 25 finished with value: -0.8578031308180915 and parameters: {'n_hidden': 72, 'dropout_rate': 0.30021754734826966}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:21,386] Trial 26 finished with value: -0.8578255721291819 and parameters: {'n_hidden': 101, 'dropout_rate': 0.26593879788782016}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:22,521] Trial 27 finished with value: -0.8564988194462724 and parameters: {'n_hidden': 63, 'dropout_rate': 0.25325779008000743}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:23,809] Trial 28 finished with value: -0.8565370557701185 and parameters: {'n_hidden': 87, 'dropout_rate': 0.2819387833735237}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:25,134] Trial 29 finished with value: -0.8591252378290597 and parameters: {'n_hidden': 101, 'dropout_rate': 0.31005874858465077}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:26,220] Trial 30 finished with value: -0.8591649486607471 and parameters: {'n_hidden': 57, 'dropout_rate': 0.22536693182260223}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:27,344] Trial 31 finished with value: -0.856570784195882 and parameters: {'n_hidden': 59, 'dropout_rate': 0.223998136974634}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:28,797] Trial 32 finished with value: -0.8578631084081683 and parameters: {'n_hidden': 75, 'dropout_rate': 0.2623572900836865}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:30,622] Trial 33 finished with value: -0.8552307370733875 and parameters: {'n_hidden': 91, 'dropout_rate': 0.24142909154657866}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:31,954] Trial 34 finished with value: -0.8564957848951393 and parameters: {'n_hidden': 76, 'dropout_rate': 0.22303754865988468}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:32,940] Trial 35 finished with value: -0.8591374523775528 and parameters: {'n_hidden': 46, 'dropout_rate': 0.22596056290093147}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:34,103] Trial 36 finished with value: -0.8552036917406517 and parameters: {'n_hidden': 73, 'dropout_rate': 0.26358049881629625}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:35,218] Trial 37 finished with value: -0.852568437230055 and parameters: {'n_hidden': 68, 'dropout_rate': 0.2424541124306249}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:36,465] Trial 38 finished with value: -0.8537456862571893 and parameters: {'n_hidden': 91, 'dropout_rate': 0.21684202796650376}. Best is trial 23 with value: -0.8604637125671057.\n",
            "[I 2023-08-28 02:55:37,392] Trial 39 finished with value: -0.8606109117499833 and parameters: {'n_hidden': 37, 'dropout_rate': 0.2184135275621624}. Best is trial 39 with value: -0.8606109117499833.\n",
            "[I 2023-08-28 02:55:38,313] Trial 40 finished with value: -0.8605576601641595 and parameters: {'n_hidden': 33, 'dropout_rate': 0.21793414125693883}. Best is trial 39 with value: -0.8606109117499833.\n",
            "[I 2023-08-28 02:55:39,200] Trial 41 finished with value: -0.8618464664139069 and parameters: {'n_hidden': 32, 'dropout_rate': 0.21168504579621306}. Best is trial 41 with value: -0.8618464664139069.\n",
            "[I 2023-08-28 02:55:40,079] Trial 42 finished with value: -0.8591427449347467 and parameters: {'n_hidden': 32, 'dropout_rate': 0.21029744679432333}. Best is trial 41 with value: -0.8618464664139069.\n",
            "[I 2023-08-28 02:55:41,074] Trial 43 finished with value: -0.8631641812680082 and parameters: {'n_hidden': 37, 'dropout_rate': 0.24070429449612527}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:42,457] Trial 44 finished with value: -0.8604808644485256 and parameters: {'n_hidden': 38, 'dropout_rate': 0.23816897368875073}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:43,866] Trial 45 finished with value: -0.8551396125507006 and parameters: {'n_hidden': 38, 'dropout_rate': 0.24073092875739086}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:44,892] Trial 46 finished with value: -0.8564190360318207 and parameters: {'n_hidden': 37, 'dropout_rate': 0.21450547820479626}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:45,923] Trial 47 finished with value: -0.8565084328283481 and parameters: {'n_hidden': 44, 'dropout_rate': 0.236855159801191}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:46,876] Trial 48 finished with value: -0.8550675124511541 and parameters: {'n_hidden': 38, 'dropout_rate': 0.21704011970926515}. Best is trial 43 with value: -0.8631641812680082.\n",
            "[I 2023-08-28 02:55:47,855] Trial 49 finished with value: -0.8565305675720402 and parameters: {'n_hidden': 48, 'dropout_rate': 0.25169777102740387}. Best is trial 43 with value: -0.8631641812680082.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Trial:\n",
            "Value:  -0.8631641812680082\n",
            "Params: \n",
            "    n_hidden: 37\n",
            "    dropout_rate: 0.24070429449612527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'sci.med']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lemmatization function using NLTK\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to the text data\n",
        "X_train_lemmatized = [lemmatize_text(text) for text in X_train]\n",
        "X_test_lemmatized = [lemmatize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lemmatized data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
        "\n",
        "# Define the PyTorch neural network architecture with optimized hyperparameters\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, dropout_rate, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(input_size, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the NeuralNetwork with the best hyperparameters obtained from Optuna\n",
        "best_n_hidden = 37  # Replace with the best value obtained from Optuna\n",
        "best_dropout_rate = 0.24070429449612527  # Replace with the best value obtained from Optuna\n",
        "\n",
        "input_size = X_train_tfidf.shape[1]\n",
        "output_size = len(np.unique(y_train))\n",
        "\n",
        "model = NeuralNetwork(input_size, best_n_hidden, best_dropout_rate, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i in range(0, X_train_tfidf.shape[0], batch_size):\n",
        "        inputs = torch.FloatTensor(X_train_tfidf[i:i+batch_size].toarray())\n",
        "        labels = torch.LongTensor(y_train[i:i+batch_size])\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    for i in range(0, X_test_tfidf.shape[0], batch_size):\n",
        "        inputs = torch.FloatTensor(X_test_tfidf[i:i+batch_size].toarray())\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_pred.extend(predicted.tolist())\n",
        "\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"Test Weighted F1-score:\", test_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6PlDM8OEv93",
        "outputId": "225944d3-6054-429d-ede4-1913fd7f4523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Weighted F1-score: 0.855017112861199\n"
          ]
        }
      ]
    }
  ]
}